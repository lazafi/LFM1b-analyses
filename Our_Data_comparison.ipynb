{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "cb8163a1-c654-4fbf-ae6a-3cfa29005e8e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1924,
    "execution_start": 1643379696694,
    "source_hash": "7ea4ef7d"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import tabulate\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from surprise import AlgoBase\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "item_threshold = 1 # 1 means no filtering\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "predict_col = 'artist'\n",
    "top_fraction = 0.2\n",
    "algo_names = ['Random',\n",
    "                'MostPopular',\n",
    "                'UserItemAvg',\n",
    "                'UserKNN',\n",
    "                'UserKNNAvg',\n",
    "                'NMF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(data_src):\n",
    "    user_events_file = (\"data/%s/user_events.txt\" % data_src)\n",
    "    low_user_file = (\"data/%s/low_main_users.txt\" % data_src)\n",
    "    medium_user_file = (\"data/%s/medium_main_users.txt\" % data_src)\n",
    "    high_user_file = (\"data/%s/high_main_users.txt\" % data_src)\n",
    "    low_users = pd.read_csv(low_user_file, sep=',').set_index('user_id')\n",
    "    medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user_id')\n",
    "    high_users = pd.read_csv(high_user_file, sep=',').set_index('user_id')\n",
    "    # read user events\n",
    "    cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "    df_events = pd.read_csv(user_events_file, sep='\\t', names=cols)\n",
    "    df_events = df_events.groupby(['user', predict_col]).size().reset_index(name='count')\n",
    "    df_events = df_events[df_events['count'] >= item_threshold]\n",
    "    print('No. filtered user events: ' + str(len(df_events)))\n",
    "    print('No. filtered items: ' + str(len(df_events[predict_col].unique())))\n",
    "    scaled_df_events = pd.DataFrame()\n",
    "    for user_id, group in df_events.groupby('user'):\n",
    "        min_rating = group['count'].min()\n",
    "        max_rating = group['count'].max()\n",
    "        scaler = MinMaxScaler(feature_range=(1, 1000))\n",
    "        scaled_ratings = scaler.fit_transform(group['count'].values.reshape(-1, 1).astype(float))\n",
    "        new_rows = group.copy()\n",
    "        new_rows['count'] = scaled_ratings\n",
    "        scaled_df_events = scaled_df_events.append(new_rows)\n",
    "    df_events = scaled_df_events\n",
    "    print('Min rating: ' + str(df_events['count'].min()))\n",
    "    print('Max rating: ' + str(df_events['count'].max()))\n",
    "    reader = Reader(rating_scale=(df_events['count'].min(), df_events['count'].max()))\n",
    "    df_events.head()\n",
    "    data = Dataset.load_from_df(df_events, reader)\n",
    "    trainset, testset = train_test_split(data, test_size = 0.2, random_state = my_seed)\n",
    "    return [low_users, medium_users, high_users, trainset, testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-5477619d-2746-4a1a-96a0-8d61b2ce5f1d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1643379698624,
    "source_hash": "febfa692"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_of_groups(lu, mu, hu, predictions):\n",
    "    print('All: ')\n",
    "    accuracy.mae(predictions)\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in lu.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in mu.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "    return [low_predictions, med_predictions, high_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitmodels(trainset, testset):\n",
    "    sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "    algos = [] # Random and MostPopular is calculated by default\n",
    "    algos.append(None)#Random())\n",
    "    algos.append(None)#MostPopular())\n",
    "    algos.append(BaselineOnly())\n",
    "    algos.append(KNNBasic(sim_options = sim_users, k=40)) \n",
    "    algos.append(KNNWithMeans(sim_options = sim_users, k=40)) \n",
    "    algos.append(NMF(n_factors = 15))\n",
    "    \n",
    "    i = 0\n",
    "    restable = []\n",
    "    for i in range(0, len(algo_names)):    \n",
    "        # get accuracy for personalized approaches\n",
    "        if algo_names[i] != 'Random' and algo_names[i] != 'MostPopular':\n",
    "            print(algo_names[i])\n",
    "            algos[i].fit(trainset)\n",
    "            predictions = algos[i].test(testset)\n",
    "            restable.append(predictions)\n",
    "    return restable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_users, medium_users, high_users, trainset, testset = readdata(\"original\")\n",
    "restable = fitmodels(trainset, testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptable = []\n",
    "for predictions in restable:\n",
    "    low_predictions, med_predictions, high_predictions = get_pred_of_groups(low_users, medium_users, high_users, predictions)\n",
    "    lowMS = np.mean(low_predictions)\n",
    "    medMS = np.mean(med_predictions)\n",
    "    highMS = np.mean(high_predictions)\n",
    "    allMS = np.mean(accuracy.mae(predictions))\n",
    "    ttestLH = stats.ttest_ind(low_predictions, high_predictions)\n",
    "    ttestLM = stats.ttest_ind(low_predictions, med_predictions)\n",
    "    ttestMH = stats.ttest_ind(med_predictions, high_predictions)\n",
    "    ptable.append([lowMS, medMS, highMS, allMS, ttestLH.pvalue, ttestLM.pvalue, ttestMH.pvalue])\n",
    "\n",
    "table = tabulate.tabulate( np.array(ptable).transpose(), headers = np.array(algo_names)[list(range(2,6))], showindex = [\"LowMS\", \"MedMS\", \"HighMS\", \"All\", \"p-value Low-High\", \"p-value Low-Med\", \"p-value Med-High\"], tablefmt='latex_raw')\n",
    "print(table)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3017f847-c231-470d-84ee-d08598884085",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
